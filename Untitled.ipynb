{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99855b5a-8ebe-4183-bffe-c33efa0674c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import schedule\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import tweepy\n",
    "import praw\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71edcec-e37f-486f-8af1-64febf223f89",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting schedule\n",
      "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: schedule\n",
      "Successfully installed schedule-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2ecb5f-fc83-483d-b3d4-3f878b28b65b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.15.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting oauthlib<4,>=3.2.0 (from tweepy)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tweepy) (2.32.3)\n",
      "Collecting requests-oauthlib<3,>=1.2.0 (from tweepy)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2024.12.14)\n",
      "Downloading tweepy-4.15.0-py3-none-any.whl (99 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.2.2 requests-oauthlib-2.0.0 tweepy-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c384e147-9e65-4f70-82a3-159e4000a119",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.12.14)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3deb047e-4f01-492f-9d21-0e244649d8e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from plotly) (1.35.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from plotly) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3c1f8e-2642-4b94-9059-575cec787082",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.167.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client)\n",
      "  Downloading protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.1)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2024.12.14)\n",
      "Downloading google_api_python_client-2.167.0-py2.py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.6/13.2 MB 9.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.9/13.2 MB 7.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.2/13.2 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.5/13.2 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.3/13.2 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.3/13.2 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.7/13.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.0/13.2 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.0/13.2 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.6/13.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.30.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: uritemplate, pyasn1, protobuf, httplib2, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.5.2 google-api-core-2.24.2 google-api-python-client-2.167.0 google-auth-2.39.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.70.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-6.30.2 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a56bd5-29c7-44cd-8ee4-3f579599b8a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 981.5/981.5 kB 8.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993363 sha256=528b55dc5e8373e24dd5105790b20dace04ddb11cf9bb8d9d46f9ab1acbe60a6\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\eb\\87\\25\\2dddf1c94e1786054e25022ec5530bfed52bad86d882999c48\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "646d93b8-bd61-4627-96c4-b325dbe97e15",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep_translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from deep_translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from deep_translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.12.14)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: deep_translator\n",
      "Successfully installed deep_translator-1.11.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6852965a-880e-4548-ad65-538232b4f995",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Setting up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"policy_pulse.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a441a29-7d78-4f80-9a02-887052bd58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPulseScraper:\n",
    "    def __init__(self, use_mongodb=True, mongodb_uri=\"mongodb://localhost:27017/\"):\n",
    "        \n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        self.all_articles = []\n",
    "        self.use_mongodb = use_mongodb\n",
    "        # Setting up MongoDB\n",
    "        if use_mongodb:\n",
    "            try:\n",
    "                self.client = MongoClient(mongodb_uri)\n",
    "                self.db = self.client[\"policypulse\"]\n",
    "                self.collection = self.db[\"policy_news\"]\n",
    "                self.feedback_collection = self.db[\"user_feedback\"]\n",
    "                self.sources_collection = self.db[\"sources\"]\n",
    "                self.users_collection = self.db[\"users\"]\n",
    "                self.alerts_collection = self.db[\"alerts\"]\n",
    "                self.sentiment_collection = self.db[\"community_sentiment\"]\n",
    "                logging.info(\"MongoDB connection established\")\n",
    "                \n",
    "                # Initialize source scoring system if it doesn't exist\n",
    "                self._initialize_source_scoring()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"MongoDB connection failed: {e}\")\n",
    "                self.use_mongodb = False\n",
    "        # Creating output directory if it doesn't exist\n",
    "        if not os.path.exists('output'):\n",
    "            os.makedirs('output')\n",
    "        # Initializing Google News API if credentials exist\n",
    "        self.google_news_api = None\n",
    "        if os.path.exists('google_api_key.txt'):\n",
    "            with open('google_api_key.txt', 'r') as f:\n",
    "                api_key = f.read().strip()\n",
    "                self.google_news_api = build('customsearch', 'v1', developerKey=api_key)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4042d76d-8eb0-458e-8962-4fdde261943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialize_source_scoring(self):\n",
    "        #Initialize source credibility scores if they don't exist\n",
    "        default_sources = [\n",
    "            {\"name\": \"PIB\", \"credibility_score\": 9.0, \"update_frequency\": \"Daily\", \"last_update\": datetime.now()},\n",
    "            {\"name\": \"PRS India\", \"credibility_score\": 8.5, \"update_frequency\": \"Weekly\", \"last_update\": datetime.now()},\n",
    "            {\"name\": \"Financial Express\", \"credibility_score\": 7.5, \"update_frequency\": \"Daily\", \"last_update\": datetime.now()},\n",
    "            {\"name\": \"Moneycontrol\", \"credibility_score\": 7.0, \"update_frequency\": \"Daily\", \"last_update\": datetime.now()},\n",
    "            {\"name\": \"Livemint\", \"credibility_score\": 7.8, \"update_frequency\": \"Daily\", \"last_update\": datetime.now()},\n",
    "            {\"name\": \"Economic Times\", \"credibility_score\": 7.6, \"update_frequency\": \"Daily\", \"last_update\": datetime.now()},\n",
    "            {\"name\": \"RBI\", \"credibility_score\": 9.5, \"update_frequency\": \"Monthly\", \"last_update\": datetime.now()},\n",
    "            {\"name\": \"Google News\", \"credibility_score\": 6.5, \"update_frequency\": \"Hourly\", \"last_update\": datetime.now()},\n",
    "            {\"name\": \"Reddit\", \"credibility_score\": 5.0, \"update_frequency\": \"Real-time\", \"last_update\": datetime.now()},\n",
    "            {\"name\": \"Twitter/X\", \"credibility_score\": 4.5, \"update_frequency\": \"Real-time\", \"last_update\": datetime.now()}\n",
    "        ]\n",
    "        \n",
    "        for source in default_sources:\n",
    "            # Add source if it doesn't exist\n",
    "            if not self.sources_collection.find_one({\"name\": source[\"name\"]}):\n",
    "                self.sources_collection.insert_one(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3cf1772-8297-460f-8921-eae422d6abb5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(self, text):\n",
    "        #Clean and normalize text content\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        # Remove extra whitespace, newlines, tabs\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[\\n\\t\\r]', ' ', text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d3c9502-d65b-4b51-81ee-be06b600d196",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fetch_page(self, url, retries=3, delay=1):\n",
    "        #Fetch webpage with retry mechanism\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                # Sleep to avoid overwhelming the server\n",
    "                time.sleep(delay + random.random())\n",
    "                return BeautifulSoup(response.content, \"html.parser\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Attempt {attempt+1} failed for {url}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(delay * (attempt + 1))  # Exponential backoff\n",
    "        \n",
    "        logging.error(f\"Failed to fetch {url} after {retries} attempts\")\n",
    "        return None\n",
    "    #BeautifulSoup object or none on failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15745563-c633-4567-a974-3c39993fb789",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_pib(self, max_articles=10):\n",
    "        #Scrape from Press Information Bureau\n",
    "        logging.info(\"Scraping PIB...\")\n",
    "        URL = \"https://pib.gov.in/indexd.aspx?relid=0\"\n",
    "        soup = self.fetch_page(URL)\n",
    "        if not soup:\n",
    "            return\n",
    "        links = soup.select(\"div.linkbox a\")[:max_articles]\n",
    "        for link in links:\n",
    "            try:\n",
    "                title = self.clean_text(link.text)\n",
    "                href = \"https://pib.gov.in/\" + link.get(\"href\")\n",
    "                \n",
    "                # Fetch article page\n",
    "                article_soup = self.fetch_page(href)\n",
    "                if not article_soup:\n",
    "                    continue\n",
    "                \n",
    "                # Extract content\n",
    "                content_div = article_soup.find(\"span\", {\"id\": \"lblContent\"})\n",
    "                content = self.clean_text(content_div.text) if content_div else \"Content not available\"\n",
    "                \n",
    "                # Extract date if available\n",
    "                date_div = article_soup.find(\"span\", {\"id\": \"lblDate\"})\n",
    "                pub_date = self.clean_text(date_div.text) if date_div else datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                \n",
    "                # Get keywords or categories if available\n",
    "                category_div = article_soup.find(\"span\", {\"id\": \"lblSector\"})\n",
    "                categories = self.clean_text(category_div.text) if category_div else \"General\"\n",
    "                \n",
    "                article = {\n",
    "                    \"source\": \"PIB\",\n",
    "                    \"title\": title,\n",
    "                    \"link\": href,\n",
    "                    \"content\": content,\n",
    "                    \"publication_date\": pub_date,\n",
    "                    \"categories\": categories,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"processed\": False\n",
    "                }\n",
    "                self.all_articles.append(article)\n",
    "                logging.info(f\"PIB article scraped: {title[:40]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping PIB article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c37f2af-ae7e-4166-92ce-41d150a24aea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_prs(self, max_articles=10):\n",
    "        #Scrape from PRS India\n",
    "        logging.info(\"Scraping PRS India...\")\n",
    "        URL = \"https://prsindia.org/billtrack\"\n",
    "        \n",
    "        soup = self.fetch_page(URL)\n",
    "        if not soup:\n",
    "            return\n",
    "        \n",
    "        cards = soup.select(\"div.view-content div.views-row\")[:max_articles]\n",
    "        \n",
    "        for card in cards:\n",
    "            try:\n",
    "                title_elem = card.find(\"span\", class_=\"field-content\")\n",
    "                if not title_elem:\n",
    "                    continue\n",
    "                    \n",
    "                title = self.clean_text(title_elem.text)\n",
    "                link_elem = card.find(\"a\")\n",
    "                if not link_elem:\n",
    "                    continue\n",
    "                    \n",
    "                href = \"https://prsindia.org\" + link_elem.get(\"href\")\n",
    "                \n",
    "                # Try to get more details from the bill page\n",
    "                bill_soup = self.fetch_page(href)\n",
    "                if bill_soup:\n",
    "                    # Extract bill status\n",
    "                    status_elem = bill_soup.select_one(\".bill-status-wrapper .field-content\")\n",
    "                    status = self.clean_text(status_elem.text) if status_elem else \"Status unknown\"\n",
    "                    \n",
    "                    # Extract bill content/summary\n",
    "                    content_elem = bill_soup.select_one(\".field-name-body .field-item\")\n",
    "                    content = self.clean_text(content_elem.text) if content_elem else \"Visit PRS to read full bill summary.\"\n",
    "                else:\n",
    "                    status = \"Status unknown\"\n",
    "                    content = \"Visit PRS to read full bill summary.\"\n",
    "                \n",
    "                article = {\n",
    "                    \"source\": \"PRS India\",\n",
    "                    \"title\": title,\n",
    "                    \"link\": href,\n",
    "                    \"content\": content,\n",
    "                    \"status\": status,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"processed\": False\n",
    "                }\n",
    "                self.all_articles.append(article)\n",
    "                logging.info(f\"PRS article scraped: {title[:40]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping PRS article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c85796c9-bd30-484b-90d3-dd72af7b22bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_financial_express(self, max_articles=10):\n",
    "        #Scrape from Financial Express\n",
    "        logging.info(\"Scraping Financial Express...\")\n",
    "        URL = \"https://www.financialexpress.com/section/economy/policy/\"\n",
    "        \n",
    "        soup = self.fetch_page(URL)\n",
    "        if not soup:\n",
    "            return\n",
    "        \n",
    "        cards = soup.select(\"div.listitembx\")[:max_articles]\n",
    "        \n",
    "        for card in cards:\n",
    "            try:\n",
    "                link_elem = card.find(\"a\")\n",
    "                if not link_elem:\n",
    "                    continue\n",
    "                    \n",
    "                title = link_elem.get(\"title\")\n",
    "                href = link_elem.get(\"href\")\n",
    "                \n",
    "                # Get full article content\n",
    "                article_soup = self.fetch_page(href)\n",
    "                if article_soup:\n",
    "                    # Extract date\n",
    "                    date_elem = article_soup.select_one(\".dateandtime\")\n",
    "                    pub_date = self.clean_text(date_elem.text) if date_elem else \"Date unknown\"\n",
    "                    \n",
    "                    # Extract content\n",
    "                    content_elems = article_soup.select(\".content-body p\")\n",
    "                    content = \"\\n\".join([self.clean_text(p.text) for p in content_elems if p]) if content_elems else \"Visit article for full content.\"\n",
    "                else:\n",
    "                    pub_date = \"Date unknown\"\n",
    "                    content = \"Visit article for full content.\"\n",
    "                \n",
    "                article = {\n",
    "                    \"source\": \"Financial Express\",\n",
    "                    \"title\": title,\n",
    "                    \"link\": href,\n",
    "                    \"content\": content,\n",
    "                    \"publication_date\": pub_date,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"processed\": False\n",
    "                }\n",
    "                self.all_articles.append(article)\n",
    "                logging.info(f\"Financial Express article scraped: {title[:40]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping Financial Express article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78fc9288-6741-49c2-a257-a15a9628b0f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_moneycontrol(self, max_articles=10):\n",
    "        #Scrape from Moneycontrol\n",
    "        logging.info(\"Scraping Moneycontrol...\")\n",
    "        URL = \"https://www.moneycontrol.com/news/business/economy/\"\n",
    "        \n",
    "        soup = self.fetch_page(URL)\n",
    "        if not soup:\n",
    "            return\n",
    "        \n",
    "        items = soup.select(\"li.clearfix\")[:max_articles]\n",
    "        \n",
    "        for item in items:\n",
    "            try:\n",
    "                a_tag = item.find(\"a\")\n",
    "                if not a_tag:\n",
    "                    continue\n",
    "                    \n",
    "                title = a_tag.get(\"title\")\n",
    "                href = a_tag.get(\"href\")\n",
    "                \n",
    "                # Get full article content\n",
    "                article_soup = self.fetch_page(href)\n",
    "                if article_soup:\n",
    "                    # Extract date\n",
    "                    date_elem = article_soup.select_one(\".article_schedule\")\n",
    "                    pub_date = self.clean_text(date_elem.text) if date_elem else \"Date unknown\"\n",
    "                    \n",
    "                    # Extract content\n",
    "                    content_div = article_soup.select_one(\".content_wrapper\")\n",
    "                    paragraphs = content_div.select(\"p\") if content_div else []\n",
    "                    content = \"\\n\".join([self.clean_text(p.text) for p in paragraphs if p]) if paragraphs else \"Visit article for full economic analysis.\"\n",
    "                else:\n",
    "                    pub_date = \"Date unknown\"\n",
    "                    content = \"Visit article for full economic analysis.\"\n",
    "                \n",
    "                article = {\n",
    "                    \"source\": \"Moneycontrol\",\n",
    "                    \"title\": title,\n",
    "                    \"link\": href,\n",
    "                    \"content\": content,\n",
    "                    \"publication_date\": pub_date,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"processed\": False\n",
    "                }\n",
    "                self.all_articles.append(article)\n",
    "                logging.info(f\"Moneycontrol article scraped: {title[:40]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping Moneycontrol article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2d0996-6605-48e2-8b98-2dc79c6e7aca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_livemint(self, max_articles=10):\n",
    "        \"\"\"Scrape from Livemint\"\"\"\n",
    "        logging.info(\"Scraping Livemint...\")\n",
    "        URL = \"https://www.livemint.com/politics/policy\"\n",
    "        \n",
    "        soup = self.fetch_page(URL)\n",
    "        if not soup:\n",
    "            return\n",
    "        \n",
    "        items = soup.select(\"section.listingPage div.listingPageNews h2 a\")[:max_articles]\n",
    "        \n",
    "        for item in items:\n",
    "            try:\n",
    "                title = self.clean_text(item.text)\n",
    "                href = \"https://www.livemint.com\" + item.get(\"href\")\n",
    "                \n",
    "                # Get full article content\n",
    "                article_soup = self.fetch_page(href)\n",
    "                if article_soup:\n",
    "                    # Extract date\n",
    "                    date_elem = article_soup.select_one(\".pubtime\")\n",
    "                    pub_date = self.clean_text(date_elem.text) if date_elem else \"Date unknown\"\n",
    "                    \n",
    "                    # Extract content\n",
    "                    content_div = article_soup.select_one(\".mainArea\")\n",
    "                    paragraphs = content_div.select(\"p\") if content_div else []\n",
    "                    content = \"\\n\".join([self.clean_text(p.text) for p in paragraphs if p]) if paragraphs else \"Visit article for full policy context.\"\n",
    "                else:\n",
    "                    pub_date = \"Date unknown\"\n",
    "                    content = \"Visit article for full policy context.\"\n",
    "                \n",
    "                article = {\n",
    "                    \"source\": \"Livemint\",\n",
    "                    \"title\": title,\n",
    "                    \"link\": href,\n",
    "                    \"content\": content,\n",
    "                    \"publication_date\": pub_date,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"processed\": False\n",
    "                }\n",
    "                self.all_articles.append(article)\n",
    "                logging.info(f\"Livemint article scraped: {title[:40]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping Livemint article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6070642-6774-4dde-b636-3e56e7e9183d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_economic_times(self, max_articles=10):\n",
    "        #Scrape from Economic Times\n",
    "        logging.info(\"Scraping Economic Times...\")\n",
    "        URL = \"https://economictimes.indiatimes.com/news/economy/policy\"\n",
    "        \n",
    "        soup = self.fetch_page(URL)\n",
    "        if not soup:\n",
    "            return\n",
    "        \n",
    "        articles = soup.select(\".eachStory\")[:max_articles]\n",
    "        \n",
    "        for article in articles:\n",
    "            try:\n",
    "                title_elem = article.select_one(\"h3 a\")\n",
    "                if not title_elem:\n",
    "                    continue\n",
    "                    \n",
    "                title = self.clean_text(title_elem.text)\n",
    "                href = title_elem.get(\"href\")\n",
    "                if href and not href.startswith(\"http\"):\n",
    "                    href = \"https://economictimes.indiatimes.com\" + href\n",
    "                \n",
    "                # Get full article content\n",
    "                article_soup = self.fetch_page(href)\n",
    "                if article_soup:\n",
    "                    # Extract date\n",
    "                    date_elem = article_soup.select_one(\".publish_on\")\n",
    "                    pub_date = self.clean_text(date_elem.text) if date_elem else \"Date unknown\"\n",
    "                    \n",
    "                    # Extract content\n",
    "                    content_elems = article_soup.select(\".artText p\")\n",
    "                    content = \"\\n\".join([self.clean_text(p.text) for p in content_elems if p]) if content_elems else \"Visit article for full content.\"\n",
    "                else:\n",
    "                    pub_date = \"Date unknown\"\n",
    "                    content = \"Visit article for full content.\"\n",
    "                \n",
    "                article = {\n",
    "                    \"source\": \"Economic Times\",\n",
    "                    \"title\": title,\n",
    "                    \"link\": href,\n",
    "                    \"content\": content,\n",
    "                    \"publication_date\": pub_date,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"processed\": False\n",
    "                }\n",
    "                self.all_articles.append(article)\n",
    "                logging.info(f\"Economic Times article scraped: {title[:40]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping Economic Times article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a688c93-8c1b-4744-84fc-451e230c8953",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_rbi_policy(self, max_articles=5):\n",
    "        \"\"\"Scrape from RBI Policy Statements\"\"\"\n",
    "        logging.info(\"Scraping RBI Policy Statements...\")\n",
    "        URL = \"https://www.rbi.org.in/Scripts/BS_PressReleaseDisplay.aspx\"\n",
    "        \n",
    "        soup = self.fetch_page(URL)\n",
    "        if not soup:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # RBI site has a different structure - looking for policy-related press releases\n",
    "            press_releases = soup.select(\"table.tablebg tr\")\n",
    "            count = 0\n",
    "            \n",
    "            for release in press_releases:\n",
    "                if count >= max_articles:\n",
    "                    break\n",
    "                    \n",
    "                # Skip header rows\n",
    "                if release.find(\"th\"):\n",
    "                    continue\n",
    "                \n",
    "                cells = release.find_all(\"td\")\n",
    "                if len(cells) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # Check if it's a policy-related release\n",
    "                title = self.clean_text(cells[1].text)\n",
    "                policy_keywords = [\"monetary policy\", \"policy\", \"repo rate\", \"banking\", \"interest rate\", \"financial\", \"regulation\"]\n",
    "                \n",
    "                if not any(keyword in title.lower() for keyword in policy_keywords):\n",
    "                    continue\n",
    "                \n",
    "                date = self.clean_text(cells[0].text)\n",
    "                link_elem = cells[1].find(\"a\")\n",
    "                \n",
    "                if not link_elem:\n",
    "                    continue\n",
    "                    \n",
    "                href = \"https://www.rbi.org.in\" + link_elem.get(\"href\")\n",
    "                \n",
    "                # Try to get full content\n",
    "                content_soup = self.fetch_page(href)\n",
    "                if content_soup:\n",
    "                    content_div = content_soup.select_one(\"#aspnetForm\")\n",
    "                    content = self.clean_text(content_div.text) if content_div else \"RBI Policy Statement - visit link for details\"\n",
    "                else:\n",
    "                    content = \"RBI Policy Statement - visit link for details\"\n",
    "                \n",
    "                article = {\n",
    "                    \"source\": \"RBI\",\n",
    "                    \"title\": title,\n",
    "                    \"link\": href,\n",
    "                    \"content\": content,\n",
    "                    \"publication_date\": date,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"processed\": False\n",
    "                }\n",
    "                self.all_articles.append(article)\n",
    "                count += 1\n",
    "                logging.info(f\"RBI policy statement scraped: {title[:40]}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping RBI policy statements: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fa084a3-b132-4ca2-a6ee-1218cdf85221",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_google_news(self, max_articles=10):\n",
    "        #Scrape from Google News API\n",
    "        if not self.google_news_api:\n",
    "            logging.warning(\"Google News API not configured, skipping\")\n",
    "            return\n",
    "            \n",
    "        logging.info(\"Scraping Google News...\")\n",
    "        query_terms = [\"india policy\", \"indian economy policy\", \"india financial policy\", \"india regulatory change\"]\n",
    "        \n",
    "        try:\n",
    "            articles_found = 0\n",
    "            \n",
    "            for query in query_terms:\n",
    "                if articles_found >= max_articles:\n",
    "                    break\n",
    "                    \n",
    "                result = self.google_news_api.cse().list(\n",
    "                    q=query,\n",
    "                    cx='YOUR_CUSTOM_SEARCH_ENGINE_ID',  # Replace with your CSE ID\n",
    "                    num=max_articles - articles_found\n",
    "                ).execute()\n",
    "                \n",
    "                if 'items' not in result:\n",
    "                    continue\n",
    "                    \n",
    "                for item in result['items']:\n",
    "                    title = item.get('title', '')\n",
    "                    link = item.get('link', '')\n",
    "                    snippet = item.get('snippet', '')\n",
    "                    \n",
    "                    article = {\n",
    "                        \"source\": \"Google News\",\n",
    "                        \"title\": title,\n",
    "                        \"link\": link,\n",
    "                        \"content\": snippet,  # Could fetch full content but respecting snippets\n",
    "                        \"publication_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                        \"processed\": False\n",
    "                    }\n",
    "                    self.all_articles.append(article)\n",
    "                    articles_found += 1\n",
    "                    logging.info(f\"Google News article found: {title[:40]}...\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error with Google News API: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e709f86f-7904-4c48-81da-cc614488e122",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_community_sentiment(self):\n",
    "        #Scrape community sentiment from social media\n",
    "        self.scrape_reddit_sentiment()\n",
    "        self.scrape_twitter_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e3d2790-81a3-427f-a19d-313c9834b2e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_reddit_sentiment(self, subreddits=None, max_posts=5):\n",
    "        #Scrape sentiment from Reddit\n",
    "        if not subreddits:\n",
    "            subreddits = [\"IndiaInvestments\", \"IndianStockMarket\", \"economy\", \"investing\", \"india\"]\n",
    "            \n",
    "        try:\n",
    "            # Initialize Reddit API (requires praw.ini config or credentials)\n",
    "            reddit = praw.Reddit(client_id='YOUR_CLIENT_ID',\n",
    "                                client_secret='YOUR_CLIENT_SECRET',\n",
    "                                user_agent='PolicyPulse/1.0')\n",
    "            \n",
    "            for subreddit_name in subreddits:\n",
    "                try:\n",
    "                    subreddit = reddit.subreddit(subreddit_name)\n",
    "                    for post in subreddit.hot(limit=max_posts):\n",
    "                        # Check if post is policy related\n",
    "                        policy_keywords = [\"policy\", \"regulation\", \"government\", \"finance\", \"budget\", \n",
    "                                         \"economic\", \"fiscal\", \"taxation\", \"law\", \"bill\"]\n",
    "                        \n",
    "                        if any(keyword in post.title.lower() or keyword in post.selftext.lower() \n",
    "                              for keyword in policy_keywords):\n",
    "                            # Store sentiment data\n",
    "                            sentiment_data = {\n",
    "                                \"platform\": \"Reddit\",\n",
    "                                \"subreddit\": subreddit_name,\n",
    "                                \"post_id\": post.id,\n",
    "                                \"title\": post.title,\n",
    "                                \"content\": post.selftext,\n",
    "                                \"url\": post.url,\n",
    "                                \"score\": post.score,\n",
    "                                \"comments_count\": post.num_comments,\n",
    "                                \"created_utc\": datetime.fromtimestamp(post.created_utc).isoformat(),\n",
    "                                \"scraped_at\": datetime.now().isoformat()\n",
    "                            }\n",
    "                            \n",
    "                            # Save to MongoDB if configured\n",
    "                            if self.use_mongodb:\n",
    "                                # Avoid duplicates\n",
    "                                if not self.sentiment_collection.find_one({\"post_id\": post.id}):\n",
    "                                    self.sentiment_collection.insert_one(sentiment_data)\n",
    "                                    \n",
    "                            logging.info(f\"Scraped Reddit post: {post.title[:40]}...\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error scraping subreddit {subreddit_name}: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing Reddit client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de5189b9-ccb2-45a1-bbe1-ac8806207458",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_twitter_sentiment(self, search_terms=None, max_tweets=50):\n",
    "        #Scrape sentiment from Twitter/X\n",
    "        if not search_terms:\n",
    "            search_terms = [\"IndianEconomy\", \"IndiaPolicies\", \"IndiaPolicy\", \"indianbudget\", \"RBIpolicy\"]\n",
    "            \n",
    "        try:\n",
    "            # Initialize Twitter API (requires API credentials)\n",
    "            auth = tweepy.OAuthHandler(\"YOUR_CONSUMER_KEY\", \"YOUR_CONSUMER_SECRET\")\n",
    "            auth.set_access_token(\"YOUR_ACCESS_TOKEN\", \"YOUR_ACCESS_TOKEN_SECRET\")\n",
    "            api = tweepy.API(auth)\n",
    "            \n",
    "            for term in search_terms:\n",
    "                try:\n",
    "                    tweets = tweepy.Cursor(api.search_tweets, q=term, tweet_mode=\"extended\", lang=\"en\").items(max_tweets)\n",
    "                    \n",
    "                    for tweet in tweets:\n",
    "                        sentiment_data = {\n",
    "                            \"platform\": \"Twitter/X\",\n",
    "                            \"tweet_id\": str(tweet.id),\n",
    "                            \"search_term\": term,\n",
    "                            \"username\": tweet.user.screen_name,\n",
    "                            \"content\": tweet.full_text,\n",
    "                            \"hashtags\": [ht['text'] for ht in tweet.entities.get('hashtags', [])],\n",
    "                            \"retweet_count\": tweet.retweet_count,\n",
    "                            \"favorite_count\": tweet.favorite_count,\n",
    "                            \"created_at\": tweet.created_at.isoformat(),\n",
    "                            \"scraped_at\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        \n",
    "                        # Save to MongoDB if configured\n",
    "                        if self.use_mongodb:\n",
    "                            # Avoid duplicates\n",
    "                            if not self.sentiment_collection.find_one({\"tweet_id\": str(tweet.id)}):\n",
    "                                self.sentiment_collection.insert_one(sentiment_data)\n",
    "                                \n",
    "                        logging.info(f\"Scraped tweet from {tweet.user.screen_name}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error searching Twitter for {term}: {e}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing Twitter client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cbbb725-c62d-4e22-84cf-5838d5dbdaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_source_scores(self):\n",
    "        #Update source credibility scores based on user feedback\n",
    "        if not self.use_mongodb:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Get all feedback\n",
    "            feedbacks = list(self.feedback_collection.find())\n",
    "            \n",
    "            # Group by source\n",
    "            source_feedback = {}\n",
    "            for feedback in feedbacks:\n",
    "                source = feedback.get(\"source\")\n",
    "                if not source:\n",
    "                    continue\n",
    "                    \n",
    "                if source not in source_feedback:\n",
    "                    source_feedback[source] = []\n",
    "                    \n",
    "                source_feedback[source].append(feedback.get(\"helpful\", False))\n",
    "                \n",
    "            # Update source scores\n",
    "            for source, feedbacks in source_feedback.items():\n",
    "                if not feedbacks:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate positive feedback percentage\n",
    "                positive_ratio = sum(feedbacks) / len(feedbacks)\n",
    "                \n",
    "                # Update source score (blend of previous score and new feedback)\n",
    "                source_doc = self.sources_collection.find_one({\"name\": source})\n",
    "                if source_doc:\n",
    "                    current_score = source_doc.get(\"credibility_score\", 5.0)\n",
    "                    # Blend with 80% old score, 20% new feedback\n",
    "                    new_score = (0.8 * current_score) + (2.0 * positive_ratio)\n",
    "                    # Keep score between 1-10\n",
    "                    new_score = max(1.0, min(10.0, new_score))\n",
    "                    \n",
    "                    # Update score\n",
    "                    self.sources_collection.update_one(\n",
    "                        {\"name\": source},\n",
    "                        {\"$set\": {\"credibility_score\": new_score, \"last_update\": datetime.now()}}\n",
    "                    )\n",
    "                    logging.info(f\"Updated score for {source}: {current_score} -> {new_score}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating source scores: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dad3c32-f1db-4d90-9147-ee47209f8722",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_to_mongodb(self, articles=None):\n",
    "        #Save articles to MongoDB\n",
    "        if not self.use_mongodb:\n",
    "            logging.warning(\"MongoDB not configured, skipping DB save\")\n",
    "            return\n",
    "            \n",
    "        articles_to_save = articles if articles is not None else self.all_articles\n",
    "            \n",
    "        try:\n",
    "            for article in articles_to_save:\n",
    "                # Check if article already exists to avoid duplicates\n",
    "                existing = self.collection.find_one({\"link\": article[\"link\"]})\n",
    "                if not existing:\n",
    "                    self.collection.insert_one(article)\n",
    "                    \n",
    "            logging.info(f\"Saved {len(articles_to_save)} articles to MongoDB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving to MongoDB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5d0cb6a-1a10-4a5e-ac7c-bf3b04582e77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_to_csv(self, filename=\"output/policy_articles.csv\", articles=None):\n",
    "        #Save articles to CSV file\n",
    "        articles_to_save = articles if articles is not None else self.all_articles\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(articles_to_save)\n",
    "            df.to_csv(filename, index=False)\n",
    "            logging.info(f\"Saved {len(df)} articles to {filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f762697-c32c-4aa2-9286-48dd9089b8d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_to_json(self, filename=\"output/policy_articles.json\", articles=None):\n",
    "        #Save articles to JSON file\n",
    "        articles_to_save = articles if articles is not None else self.all_articles\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(articles_to_save)\n",
    "            df.to_json(filename, orient=\"records\", indent=4)\n",
    "            logging.info(f\"Saved {len(df)} articles to {filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving to JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0de5a955-a403-4bc8-9909-5f9cf6232c71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_all_scrapers(self, articles_per_source=5, process_with_nlp=True, include_community=True):\n",
    "        #Run all scrapers with option to process with NLP\n",
    "        logging.info(\"Starting Policy Pulse scraping job\")\n",
    "        \n",
    "        # Run all scrapers\n",
    "        self.scrape_pib(articles_per_source)\n",
    "        self.scrape_prs(articles_per_source)\n",
    "        self.scrape_financial_express(articles_per_source)\n",
    "        self.scrape_moneycontrol(articles_per_source)\n",
    "        self.scrape_livemint(articles_per_source)\n",
    "        self.scrape_economic_times(articles_per_source)\n",
    "        self.scrape_rbi_policy(articles_per_source)\n",
    "        self.scrape_google_news(articles_per_source)\n",
    "    # Add community sentiment if requested\n",
    "        if include_community:\n",
    "            self.scrape_community_sentiment()\n",
    "            \n",
    "        # Process articles with NLP if requested\n",
    "        if process_with_nlp:\n",
    "            self.process_articles_with_nlp()\n",
    "            \n",
    "        # Save the articles\n",
    "        self.save_to_mongodb()\n",
    "        self.save_to_csv()\n",
    "        self.save_to_json()\n",
    "        \n",
    "        # Update source scoring\n",
    "        self.update_source_scores()\n",
    "        \n",
    "        logging.info(f\"Policy Pulse scraping job completed with {len(self.all_articles)} articles\")\n",
    "        return self.all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b286fb0b-7ec8-433c-ba8c-e68f8358982c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    " def process_articles_with_nlp(self):\n",
    "        \"\"\"Process articles with NLP for sentiment, topics, and importance\"\"\"\n",
    "        logging.info(\"Processing articles with NLP...\")\n",
    "        \n",
    "        # Check if we have articles to process\n",
    "        if not self.all_articles:\n",
    "            logging.warning(\"No articles to process with NLP\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Initialize NLP pipelines\n",
    "            sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "            summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "            zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "            \n",
    "            policy_categories = [\n",
    "                \"Taxation\", \"Banking\", \"Finance\", \"Agriculture\", \"Industry\",\n",
    "                \"Infrastructure\", \"Education\", \"Healthcare\", \"Environment\", \n",
    "                \"Foreign Trade\", \"Labor\", \"Digital\", \"Monetary Policy\"\n",
    "            ]\n",
    "            \n",
    "            for i, article in enumerate(self.all_articles):\n",
    "                if article.get(\"processed\"):\n",
    "                    continue\n",
    "                    \n",
    "                # Skip if no content\n",
    "                if not article.get(\"content\") or len(article[\"content\"]) < 50:\n",
    "                    continue\n",
    "                    \n",
    "                logging.info(f\"Processing article {i+1}/{len(self.all_articles)}: {article['title'][:30]}...\")\n",
    "                \n",
    "                # Process content\n",
    "                content = article[\"content\"]\n",
    "                \n",
    "                # Detect language and translate if not English\n",
    "                try:\n",
    "                    lang = detect(content[:100])\n",
    "                    if lang != 'en':\n",
    "                        translator = GoogleTranslator(source=lang, target='en')\n",
    "                        content = translator.translate(content[:5000])  # Limit to avoid API issues\n",
    "                        article[\"original_language\"] = lang\n",
    "                except:\n",
    "                    pass  # Continue with original content if language detection fails\n",
    "                \n",
    "                # Sentiment analysis\n",
    "                try:\n",
    "                    # Use first 512 tokens to avoid model limits\n",
    "                    shortened_content = ' '.join(content.split()[:512])\n",
    "                    sentiment_result = sentiment_analyzer(shortened_content)\n",
    "                    article[\"sentiment\"] = sentiment_result[0]\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Sentiment analysis failed: {e}\")\n",
    "                    article[\"sentiment\"] = {\"label\": \"unknown\", \"score\": 0.0}\n",
    "                \n",
    "                # Summarization (if content is long enough)\n",
    "                try:\n",
    "                    if len(content.split()) > 100:\n",
    "                        # Use first 1024 tokens to avoid model limits\n",
    "                        shortened_content = ' '.join(content.split()[:1024])\n",
    "                        summary = summarizer(shortened_content, max_length=150, min_length=30, do_sample=False)\n",
    "                        article[\"summary\"] = summary[0][\"summary_text\"]\n",
    "                    else:\n",
    "                        article[\"summary\"] = content[:200] + \"...\"\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Summarization failed: {e}\")\n",
    "                    article[\"summary\"] = content[:200] + \"...\"\n",
    "                \n",
    "                # Topic classification\n",
    "                try:\n",
    "                    # Use first 512 tokens to avoid model limits\n",
    "                    shortened_content = ' '.join(content.split()[:512])\n",
    "                    topic_result = zero_shot_classifier(\n",
    "                        shortened_content, \n",
    "                        candidate_labels=policy_categories,\n",
    "                        multi_label=True\n",
    "                    )\n",
    "                    \n",
    "                    # Keep only categories with decent confidence\n",
    "                    relevant_topics = [\n",
    "                        {\"category\": label, \"score\": score} \n",
    "                        for label, score in zip(topic_result[\"labels\"], topic_result[\"scores\"])\n",
    "                        if score > 0.3  # Only keep categories with >30% confidence\n",
    "                    ]\n",
    "                    article[\"topics\"] = relevant_topics\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Topic classification failed: {e}\")\n",
    "                    article[\"topics\"] = []\n",
    "                \n",
    "                # Calculate importance score based on multiple factors\n",
    "                try:\n",
    "                    # Base score on source credibility\n",
    "                    if self.use_mongodb:\n",
    "                        source_doc = self.sources_collection.find_one({\"name\": article[\"source\"]})\n",
    "                        base_score = source_doc.get(\"credibility_score\", 5.0) if source_doc else 5.0\n",
    "                    else:\n",
    "                        base_score = 5.0\n",
    "                    \n",
    "                    # Adjust based on sentiment confidence\n",
    "                    sentiment_confidence = article[\"sentiment\"][\"score\"] if \"sentiment\" in article else 0.5\n",
    "                    \n",
    "                    # Adjust based on topic relevance\n",
    "                    topic_score = 0\n",
    "                    if \"topics\" in article and article[\"topics\"]:\n",
    "                        topic_score = max([t[\"score\"] for t in article[\"topics\"]]) if article[\"topics\"] else 0.5\n",
    "                    \n",
    "                    # Calculate final importance score (1-10 scale)\n",
    "                    importance = (0.5 * base_score) + (2 * sentiment_confidence) + (3 * topic_score)\n",
    "                    importance = max(1.0, min(10.0, importance))  # Clamp between 1-10\n",
    "                    article[\"importance_score\"] = round(importance, 2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Importance scoring failed: {e}\")\n",
    "                    article[\"importance_score\"] = 5.0\n",
    "                \n",
    "                # Mark as processed\n",
    "                article[\"processed\"] = True\n",
    "                \n",
    "                # Update in MongoDB if configured\n",
    "                if self.use_mongodb:\n",
    "                    self.collection.update_one(\n",
    "                        {\"_id\": article.get(\"_id\", ObjectId())},\n",
    "                        {\"$set\": article}\n",
    "                    )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in NLP processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ef0c985-b30c-40d1-8662-ebbaa7e4b1b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_daily_digest(self, top_n=10, output_format=\"html\"):\n",
    "        #Generate a daily digest of the most important policy updates\n",
    "        logging.info(\"Generating daily digest...\")\n",
    "        \n",
    "        # Get most recent articles\n",
    "        last_24h = datetime.now() - timedelta(hours=24)\n",
    "        \n",
    "        if self.use_mongodb:\n",
    "            recent_articles = list(self.collection.find({\n",
    "                \"timestamp\": {\"$gte\": last_24h.isoformat()}\n",
    "            }).sort(\"importance_score\", -1).limit(top_n))\n",
    "        else:\n",
    "            # Filter from all_articles\n",
    "            recent_articles = [a for a in self.all_articles \n",
    "                              if datetime.fromisoformat(a.get(\"timestamp\", \"2000-01-01\")) >= last_24h]\n",
    "            # Sort by importance\n",
    "            recent_articles.sort(key=lambda x: x.get(\"importance_score\", 0), reverse=True)\n",
    "            recent_articles = recent_articles[:top_n]\n",
    "        \n",
    "        if not recent_articles:\n",
    "            logging.warning(\"No recent articles for digest\")\n",
    "            return None\n",
    "            \n",
    "        # Generate the digest\n",
    "        if output_format.lower() == \"html\":\n",
    "            return self._generate_html_digest(recent_articles)\n",
    "        elif output_format.lower() == \"markdown\":\n",
    "            return self._generate_markdown_digest(recent_articles)\n",
    "        elif output_format.lower() == \"json\":\n",
    "            return self._generate_json_digest(recent_articles)\n",
    "        else:\n",
    "            logging.error(f\"Unknown digest format: {output_format}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8063a-f7f2-4fd2-8a92-8a276ec0bed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd064c19-9fc1-4fa8-b944-7b84bdbf87b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _generate_markdown_digest(self, articles):\n",
    "        \"\"\"Generate Markdown digest\"\"\"\n",
    "        markdown = f\"# Policy Pulse - Daily Digest\\n\\n\"\n",
    "        markdown += f\"Top policy updates as of {datetime.now().strftime('%d %B, %Y')}\\n\\n\"\n",
    "        \n",
    "        for article in articles:\n",
    "            markdown += f\"## [{article.get('title', 'Untitled')}]({article.get('link', '#')})\\n\\n\"\n",
    "            markdown += f\"**Source**: {article.get('source', 'Unknown Source')}  \\n\"\n",
    "            markdown += f\"**Date**: {article.get('publication_date', 'Date unknown')}  \\n\"\n",
    "            markdown += f\"**Sentiment**: {article.get('sentiment', {}).get('label', 'neutral')}  \\n\"\n",
    "            \n",
    "            # Add topics\n",
    "            if \"topics\" in article and article[\"topics\"]:\n",
    "                topics_str = \", \".join([t[\"category\"] for t in article[\"topics\"][:3]])\n",
    "                markdown += f\"**Topics**: {topics_str}  \\n\"\n",
    "                \n",
    "            markdown += f\"\\n{article.get('summary', article.get('content', '')[:200]+'...')}\\n\\n\"\n",
    "            markdown += \"---\\n\\n\"\n",
    "            \n",
    "        # Save to file\n",
    "        with open(\"output/daily_digest.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown)\n",
    "            \n",
    "        return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d1ab2f3-5f6e-44fb-8a3d-9076bc4aa9ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _generate_json_digest(self, articles):\n",
    "        \"\"\"Generate JSON digest\"\"\"\n",
    "        # Create simplified version of articles\n",
    "        digest_articles = []\n",
    "        \n",
    "        for article in articles:\n",
    "            digest_article = {\n",
    "                \"title\": article.get(\"title\", \"Untitled\"),\n",
    "                \"source\": article.get(\"source\", \"Unknown Source\"),\n",
    "                \"link\": article.get(\"link\", \"#\"),\n",
    "                \"publication_date\": article.get(\"publication_date\", \"Date unknown\"),\n",
    "                \"summary\": article.get(\"summary\", article.get(\"content\", \"\")[:200]+\"...\"),\n",
    "                \"sentiment\": article.get(\"sentiment\", {}).get(\"label\", \"neutral\"),\n",
    "                \"importance_score\": article.get(\"importance_score\", 5.0)\n",
    "            }\n",
    "            \n",
    "            # Add topics if available\n",
    "            if \"topics\" in article and article[\"topics\"]:\n",
    "                digest_article[\"topics\"] = [t[\"category\"] for t in article[\"topics\"][:3]]\n",
    "                \n",
    "            digest_articles.append(digest_article)\n",
    "            \n",
    "        digest = {\n",
    "            \"title\": \"Policy Pulse - Daily Digest\",\n",
    "            \"date\": datetime.now().isoformat(),\n",
    "            \"article_count\": len(digest_articles),\n",
    "            \"articles\": digest_articles\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(\"output/daily_digest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(digest, f, indent=2)\n",
    "            \n",
    "        return digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08762a71-201e-4ca9-9831-a6a620bb610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email_digest(self, recipients, subject=None, html_digest=None):\n",
    "        \"\"\"Send email digest to specified recipients\"\"\"\n",
    "        if not html_digest:\n",
    "            html_digest = self.generate_daily_digest(output_format=\"html\")\n",
    "            \n",
    "        if not html_digest:\n",
    "            logging.error(\"No digest content to email\")\n",
    "            return False\n",
    "            \n",
    "        if not subject:\n",
    "            subject = f\"Policy Pulse Digest - {datetime.now().strftime('%d %B, %Y')}\"\n",
    "            \n",
    "        try:\n",
    "            # Email server settings\n",
    "            smtp_server = \"smtp.gmail.com\"  # Replace with your SMTP server\n",
    "            smtp_port = 587\n",
    "            sender_email = \"your_email@example.com\"  # Replace with your email\n",
    "            password = os.environ.get(\"EMAIL_PASSWORD\", \"\")  # Get from environment variable for security\n",
    "            \n",
    "            if not password:\n",
    "                logging.error(\"No email password provided in environment variable EMAIL_PASSWORD\")\n",
    "                return False\n",
    "                \n",
    "            # Create message\n",
    "            message = MIMEMultipart(\"alternative\")\n",
    "            message[\"Subject\"] = subject\n",
    "            message[\"From\"] = sender_email\n",
    "            message[\"To\"] = \", \".join(recipients)\n",
    "            \n",
    "            # Add HTML content\n",
    "            part = MIMEText(html_digest, \"html\")\n",
    "            message.attach(part)\n",
    "            \n",
    "            # Connect to server and send\n",
    "            with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
    "                server.starttls()\n",
    "                server.login(sender_email, password)\n",
    "                server.sendmail(sender_email, recipients, message.as_string())\n",
    "                \n",
    "            logging.info(f\"Email digest sent to {len(recipients)} recipients\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error sending email digest: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ce4fd75-f5de-40c6-bb44-bd5b96b9ca5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def handle_user_alerts(self):\n",
    "        \"\"\"Process and send user alerts for policy updates matching their interests\"\"\"\n",
    "        if not self.use_mongodb:\n",
    "            logging.warning(\"MongoDB not configured, skipping alerts\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Get all alerts\n",
    "            alerts = list(self.alerts_collection.find())\n",
    "            \n",
    "            # Get recent articles (last 24 hours)\n",
    "            last_24h = datetime.now() - timedelta(hours=24)\n",
    "            recent_articles = list(self.collection.find({\n",
    "                \"timestamp\": {\"$gte\": last_24h.isoformat()}\n",
    "            }))\n",
    "            \n",
    "            if not recent_articles:\n",
    "                logging.info(\"No recent articles for alerts\")\n",
    "                return\n",
    "                \n",
    "            # Process each alert\n",
    "            for alert in alerts:\n",
    "                user_id = alert.get(\"user_id\")\n",
    "                user = self.users_collection.find_one({\"_id\": ObjectId(user_id)})\n",
    "                \n",
    "                if not user:\n",
    "                    logging.warning(f\"User not found for alert {alert.get('_id')}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Get user email\n",
    "                user_email = user.get(\"email\")\n",
    "                if not user_email:\n",
    "                    continue\n",
    "                    \n",
    "                # Get alert keywords and topics\n",
    "                keywords = alert.get(\"keywords\", [])\n",
    "                topics = alert.get(\"topics\", [])\n",
    "                min_importance = alert.get(\"min_importance\", 7.0)  # Default threshold\n",
    "                \n",
    "                # Find matching articles\n",
    "                matching_articles = []\n",
    "                \n",
    "                for article in recent_articles:\n",
    "                    # Check importance threshold\n",
    "                    if article.get(\"importance_score\", 0) < min_importance:\n",
    "                        continue\n",
    "                        \n",
    "                    # Check keywords\n",
    "                    title = article.get(\"title\", \"\").lower()\n",
    "                    content = article.get(\"content\", \"\").lower()\n",
    "                    \n",
    "                    keyword_match = any(kw.lower() in title or kw.lower() in content \n",
    "                                       for kw in keywords if kw)\n",
    "                                       \n",
    "                    # Check topics\n",
    "                    article_topics = [t[\"category\"] for t in article.get(\"topics\", [])]\n",
    "                    topic_match = any(topic in article_topics for topic in topics if topic)\n",
    "                    \n",
    "                    # Add if matching\n",
    "                    if keyword_match or topic_match:\n",
    "                        matching_articles.append(article)\n",
    "                \n",
    "                # Send alert if we have matches\n",
    "                if matching_articles:\n",
    "                    # Generate HTML digest for matching articles\n",
    "                    html_digest = self._generate_html_digest(matching_articles)\n",
    "                    \n",
    "                    # Send email\n",
    "                    subject = f\"Policy Pulse Alert - {len(matching_articles)} new matching updates\"\n",
    "                    self.send_email_digest([user_email], subject, html_digest)\n",
    "                    \n",
    "                    logging.info(f\"Sent alert to {user_email} with {len(matching_articles)} matching articles\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing user alerts: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed4af9e7-901c-4449-a91c-aaf83105431b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_sentiment_trends(self, days=30, output_file=\"output/sentiment_trends.html\"):\n",
    "        \"\"\"Create visualization of sentiment trends over time\"\"\"\n",
    "        if not self.use_mongodb:\n",
    "            logging.warning(\"MongoDB not configured, skipping visualization\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Get articles from last N days\n",
    "            start_date = datetime.now() - timedelta(days=days)\n",
    "            articles = list(self.collection.find({\n",
    "                \"timestamp\": {\"$gte\": start_date.isoformat()},\n",
    "                \"sentiment\": {\"$exists\": True}\n",
    "            }))\n",
    "            \n",
    "            if not articles:\n",
    "                logging.warning(\"No articles with sentiment for visualization\")\n",
    "                return\n",
    "                \n",
    "            # Prepare data\n",
    "            dates = []\n",
    "            positive_counts = []\n",
    "            negative_counts = []\n",
    "            neutral_counts = []\n",
    "            \n",
    "            # Group by date\n",
    "            date_grouped = {}\n",
    "            for article in articles:\n",
    "                # Get date from timestamp or publication_date\n",
    "                try:\n",
    "                    article_date = datetime.fromisoformat(article.get(\"timestamp\")).strftime(\"%Y-%m-%d\")\n",
    "                except:\n",
    "                    try:\n",
    "                        article_date = datetime.fromisoformat(article.get(\"publication_date\")).strftime(\"%Y-%m-%d\")\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Initialize if new date\n",
    "                if article_date not in date_grouped:\n",
    "                    date_grouped[article_date] = {\"POSITIVE\": 0, \"NEGATIVE\": 0, \"NEUTRAL\": 0}\n",
    "                \n",
    "                # Get sentiment\n",
    "                sentiment = article.get(\"sentiment\", {}).get(\"label\", \"NEUTRAL\")\n",
    "                date_grouped[article_date][sentiment] += 1\n",
    "            \n",
    "            # Sort by date\n",
    "            sorted_dates = sorted(date_grouped.keys())\n",
    "            \n",
    "            for date in sorted_dates:\n",
    "                dates.append(date)\n",
    "                positive_counts.append(date_grouped[date][\"POSITIVE\"])\n",
    "                negative_counts.append(date_grouped[date][\"NEGATIVE\"])\n",
    "                neutral_counts.append(date_grouped[date][\"NEUTRAL\"])\n",
    "            \n",
    "            # Create plotly figure\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=dates, y=positive_counts,\n",
    "                name='Positive', mode='lines+markers',\n",
    "                line=dict(color='green', width=2)\n",
    "            ))\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=dates, y=negative_counts,\n",
    "                name='Negative', mode='lines+markers',\n",
    "                line=dict(color='red', width=2)\n",
    "            ))\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=dates, y=neutral_counts,\n",
    "                name='Neutral', mode='lines+markers',\n",
    "                line=dict(color='gray', width=2)\n",
    "            ))\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                title='Policy Sentiment Trends Over Time',\n",
    "                xaxis_title='Date',\n",
    "                yaxis_title='Number of Articles',\n",
    "                legend_title='Sentiment',\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            \n",
    "            # Save to file\n",
    "            fig.write_html(output_file)\n",
    "            logging.info(f\"Sentiment trends visualization saved to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating sentiment visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c36a46b-46d0-4adb-94e9-4697e548635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_scraper(mongodb_uri=\"mongodb://localhost:27017/\", run_interval_hours=24, digest_recipients=None):\n",
    "    \"\"\"\n",
    "    Schedule the scraper to run at regular intervals\n",
    "    \n",
    "    Args:\n",
    "        mongodb_uri: MongoDB connection string\n",
    "        run_interval_hours: How often to run in hours\n",
    "        digest_recipients: List of email addresses to receive digest\n",
    "    \"\"\"\n",
    "    if digest_recipients is None:\n",
    "        digest_recipients = []\n",
    "        \n",
    "    def scraper_job():\n",
    "        \"\"\"Job to run at scheduled intervals\"\"\"\n",
    "        logging.info(\"Starting scheduled Policy Pulse job\")\n",
    "        scraper = PolicyPulseScraper(use_mongodb=True, mongodb_uri=mongodb_uri)\n",
    "        scraper.run_all_scrapers(articles_per_source=10, process_with_nlp=True)\n",
    "        \n",
    "        # Generate and email digest if recipients\n",
    "        if digest_recipients:\n",
    "            scraper.send_email_digest(digest_recipients)\n",
    "            \n",
    "        # Process user alerts\n",
    "        scraper.handle_user_alerts()\n",
    "        \n",
    "        # Generate visualization\n",
    "        scraper.visualize_sentiment_trends()\n",
    "        \n",
    "        logging.info(\"Scheduled Policy Pulse job completed\")\n",
    "    \n",
    "    # Run once immediately\n",
    "    scraper_job()\n",
    "    \n",
    "    # Schedule to run at intervals\n",
    "    schedule.every(run_interval_hours).hours.do(scraper_job)\n",
    "    \n",
    "    logging.info(f\"Scheduled to run every {run_interval_hours} hours\")\n",
    "    \n",
    "    # Keep running\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e929b7fc-8020-4fd2-b5ec-4d03198b2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPulseScraper:\n",
    "    def __init__(self, use_mongodb=True, mongodb_uri=\"mongodb://localhost:27017/\"):\n",
    "        self.use_mongodb = use_mongodb\n",
    "        self.mongodb_uri = mongodb_uri\n",
    "        self.db = None\n",
    "\n",
    "        if self.use_mongodb:\n",
    "            from pymongo import MongoClient\n",
    "            self.db = MongoClient(mongodb_uri)[\"policy_pulse\"]\n",
    "\n",
    "        # Ensure this exists if your code uses it\n",
    "        self._initialize_source_scoring()\n",
    "\n",
    "    def _initialize_source_scoring(self):\n",
    "        # Dummy method for now  update with actual logic later\n",
    "        pass\n",
    "\n",
    "    def run_all_scrapers(self, articles_per_source=5, process_with_nlp=True, include_community=True):\n",
    "        # Add your real scraping logic here\n",
    "        print(\"Running all scrapers...\")\n",
    "        articles = []  # Replace with actual scraped data\n",
    "        return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c111800f-b1a2-4b79-86a1-c8ad08dd9b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running all scrapers...\n"
     ]
    }
   ],
   "source": [
    " import sys\n",
    "import argparse\n",
    "\n",
    "def run_once(args):\n",
    "    \"\"\"Run scraper once with command line arguments\"\"\"\n",
    "    scraper = PolicyPulseScraper(use_mongodb=not args.no_mongodb, mongodb_uri=args.mongodb_uri)\n",
    "    articles = scraper.run_all_scrapers(\n",
    "        articles_per_source=args.articles_per_source,\n",
    "        process_with_nlp=not args.skip_nlp,\n",
    "        include_community=not args.skip_community\n",
    "    )\n",
    "    \n",
    "    # Generate digest if requested\n",
    "    if args.digest:\n",
    "        format = args.digest_format.lower()\n",
    "        scraper.generate_daily_digest(top_n=args.digest_count, output_format=format)\n",
    "        \n",
    "    # Send email if recipients are provided\n",
    "    if args.email and args.recipients:\n",
    "        recipients = args.recipients.split(\",\")\n",
    "        scraper.send_email_digest(recipients)\n",
    "        \n",
    "    # Generate visualization if requested\n",
    "    if args.visualize:\n",
    "        scraper.visualize_sentiment_trends(days=args.visualize_days)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "if __name__ == \"__main__\":  # Fixed double asterisks to double underscores\n",
    "    parser = argparse.ArgumentParser(description=\"Policy Pulse - Policy News Aggregator\")\n",
    "    parser.add_argument(\"--no-mongodb\", action=\"store_true\", help=\"Don't use MongoDB\")\n",
    "    parser.add_argument(\"--mongodb-uri\", default=\"mongodb://localhost:27017/\", help=\"MongoDB connection URI\")\n",
    "    parser.add_argument(\"--articles-per-source\", type=int, default=5, help=\"Number of articles to scrape per source\")\n",
    "    parser.add_argument(\"--skip-nlp\", action=\"store_true\", help=\"Skip NLP processing\")\n",
    "    parser.add_argument(\"--skip-community\", action=\"store_true\", help=\"Skip community sentiment scraping\")\n",
    "    parser.add_argument(\"--digest\", action=\"store_true\", help=\"Generate daily digest\")\n",
    "    parser.add_argument(\"--digest-format\", default=\"html\", choices=[\"html\", \"markdown\", \"json\"], help=\"Digest format\")\n",
    "    parser.add_argument(\"--digest-count\", type=int, default=10, help=\"Number of articles in digest\")\n",
    "    parser.add_argument(\"--email\", action=\"store_true\", help=\"Send email digest\")\n",
    "    parser.add_argument(\"--recipients\", type=str, help=\"Comma-separated email recipients\")\n",
    "    parser.add_argument(\"--visualize\", action=\"store_true\", help=\"Generate sentiment visualization\")\n",
    "    parser.add_argument(\"--visualize-days\", type=int, default=30, help=\"Days to include in visualization\")\n",
    "    parser.add_argument(\"--schedule\", action=\"store_true\", help=\"Run as scheduled job\")\n",
    "    parser.add_argument(\"--interval\", type=int, default=24, help=\"Schedule interval in hours\")\n",
    "    \n",
    "    args = parser.parse_args(args=[]) if 'ipykernel' in sys.argv[0] else parser.parse_args()\n",
    "    \n",
    "    if args.schedule:\n",
    "        # Run as scheduled job\n",
    "        recipients = args.recipients.split(\",\") if args.recipients else []\n",
    "        schedule_scraper(\n",
    "            mongodb_uri=args.mongodb_uri,\n",
    "            run_interval_hours=args.interval,\n",
    "            digest_recipients=recipients\n",
    "        )\n",
    "    else:\n",
    "        # Run once\n",
    "        run_once(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bb6249a-60fe-48d8-b4c3-18ddb3c8b062",
   "metadata": {},
   "outputs": [],
   "source": [
    " import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1eb252a-cef8-4f07-8552-34eb0c1df193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sys (from versions: none)\n",
      "ERROR: No matching distribution found for sys\n"
     ]
    }
   ],
   "source": [
    "pip install sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a6758-0469-4358-9b81-95cbf247537e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
